---
sidebar: sidebar
permalink: solutions/ontap-day01-deploy.html
keywords: bluexp automation catalog, netapp automation solutions, ontap day 0/1, deploy
summary:
---

= Deploy the ONTAP cluster
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
After completing the preparation and planning, you are ready to use the ONTAP day 0/1 solution to quickly configure an ONTAP cluster using Ansible. 

At any time during the steps in this section, you can choose to test a request instead of executing. To do this, you must change the `site.yml` playbook on the command line to use `logic.yml`. 

The `docs/tutorial-requests.txt` location contains the final version of all service requests used throughout this procedure. If you have difficulty running a service request, you can copy the relevant request from the `tutorial-requests.txt` file to the `playbooks/inventory/group_vars/all/tutorial-requests.yml` location and modify the hard-coded values as required (IP address, aggregate names and so on). You should then be able to successfully run the request. 

== Before you begin

* You must have Ansible installed. 
* You must have downloaded the ONTAP day 0/1 solution and extracted the folder to the desired location on the Ansible control node. 
* The ONTAP system state must meet the requirements and you must have the necessary credentials. 
* You must have completed all required tasks outlined in the link:ontap-day01-prepare.html[Prepare] section.

== Step 1: Initial cluster configuration

At this stage, you must perform some initial cluster configuration steps. 

. Navigate to the `playbooks/inventory/group_vars/all/tutorial-requests.yml` location and review the `cluster_initial` request in the file. Make any necessary changes for your environment. 
+
At the bottom of the file, uncomment the `raw_service_request` definition and confirm that the value is set to `cluster_initial`.
+
NOTE: The examples throughout this solution use "Cluster_01" and "Cluster_02" as the names for the two clusters. You must replace these values with the names of the clusters in your environment. 

. Perform the initial cluster configuration for the first cluster:
+
[source,cli]
ansible-playbook -i inventory/hosts site.yml -e cluster_name=<Cluster_01>
+
Verify that there are no errors before proceeding. 

. Repeat the command for the second cluster:
+
[source,cli]
ansible-playbook -i inventory/hosts site.yml -e cluster_name=<Cluster_02>
+
Verify that there are no errors for the second cluster.
+
When you scroll up towards the beginning of the Ansible output you should see the request that was sent to the framework, as shown in the following example:
+
----
TASK [Show the raw_service_request] ************************************************************************************************************
ok: [localhost] => {
    "raw_service_request": {
        "operation": "create",
        "req_details": {
            "ontap_aggr": [
                {
                    "disk_count": 24,
                    "hostname": "Cluster_01",
                    "name": "n01_aggr1",
                    "nodes": "Cluster_01-01",
                    "raid_type": "raid4"
                }
            ],
            "ontap_license": [
                {
                    "hostname": "Cluster_01",
                    "license_codes": [
                        "XXXXXXXXXXXXXXXAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA",
                        "XXXXXXXXXXXXXXAAAAAAAAAAAAA"                   
                    ]
                }
            ],
            "ontap_motd": [
                {
                    "hostname": "Cluster_01",
                    "message": "New MOTD",
                    "vserver": "Cluster_01"
                }
            ]
        },
        "service": "cluster_initial",
        "std_name": "none"
    }
}
----

. Log in to each ONTAP instance and verify that the request was successful. 

== Step 2: Configure the intercluster LIFs

You can now configure the intercluster LIFs by adding the LIF definitions to the `cluster_initial` request and defining the `ontap_interface` microservice. 

The service definition and the request work together to determine the action:

* If you provide a service request for a microservice that is not in the service definitions, the request is not executed. 

* If you provide a service request with one or more microservices defined in the service definitions, but omitted from the request, the request is not executed. 

The `execution.yml` playbook evaluates the service definition by scanning the list of microservices in the order listed:

* If there is an entry in the request with a dictionary key matching the *“args”* entry contained in the microservice definitions, the request is executed. 
* If there is no matching entry in the service request, it is skipped without error.

.Steps

. Navigate to the `tutorial-requests.yml` file and modify the `cluster_initial` request by adding the following lines to the request definitions:
+
----
    ontap_interface:
    - hostname:                   "{{ cluster_name }}"
      vserver:                    "{{ cluster_name }}"
      interface_name:             ic01
      role:                       intercluster
      address:                    <ip_address>
      netmask:                    <netmask_address>
      home_node:                  "{{ cluster_name }}-01"
      home_port:                  e0c
      ipspace:                    Default
      use_rest:                   never

    - hostname:                   "{{ cluster_name }}"
      vserver:                    "{{ cluster_name }}"
      interface_name:             ic02
      role:                       intercluster
      address:                    <ip_address>
      netmask:                    <netmask_address>
      home_node:                  "{{ cluster_name }}-01"
      home_port:                  e0c
      ipspace:                    Default
      use_rest:                   never

    - hostname:                   "{{ peer_cluster_name }}"
      vserver:                    "{{ peer_cluster_name }}"
      interface_name:             ic01
      role:                       intercluster
      address:                    <ip_address>
      netmask:                    <netmask_address>
      home_node:                  "{{ peer_cluster_name }}-01"
      home_port:                  e0c
      ipspace:                    Default
      use_rest:                   never

    - hostname:                   "{{ peer_cluster_name }}"
      vserver:                    "{{ peer_cluster_name }}"
      interface_name:             ic02
      role:                       intercluster
      address:                    <ip_address>
      netmask:                    <netmask_address>
      home_node:                  "{{ peer_cluster_name }}-01"
      home_port:                  e0c
      ipspace:                    Default
      use_rest:                   never
----

. Run the command:
+
[source, cli]
ansible-playbook -i inventory/hosts  site.yml -e cluster_name=<Cluster_01> -e peer_cluster_name=<Cluster_02>

. Log in to each instance to check if the LIFs have been added: 
+
----
Cluster_01::> net int show
  (network interface show)
            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
Cluster_01
            Cluster_01-01_mgmt up/up 10.0.0.101/24   Cluster_01-01 e0c     true
            Cluster_01-01_mgmt_auto up/up 10.101.101.101/24 Cluster_01-01 e0c true
            cluster_mgmt up/up    10.0.0.110/24      Cluster_01-01 e0c     true
5 entries were displayed.
----
+
The output shows that the LIFs were *not* added. This is because the `ontap_interface` microservice still needs to be defined in the `services.yml` file.

. Verify that the LIFs were added to the `raw_service_request`.
+
The following example shows that the LIFs have been added to the request:
+
----
           "ontap_interface": [
                {
                    "address": "10.0.0.101",
                    "home_node": "Cluster_01-01",
                    "home_port": "e0c",
                    "hostname": "Cluster_01",
                    "interface_name": "ic01",
                    "ipspace": "Default",
                    "netmask": "255.255.255.0",
                    "role": "intercluster",
                    "use_rest": "never",
                    "vserver": "Cluster_01"
                },
                {
                    "address": "10.0.0.101",
                    "home_node": "Cluster_01-01",
                    "home_port": "e0c",
                    "hostname": "Cluster_01",
                    "interface_name": "ic02",
                    "ipspace": "Default",
                    "netmask": "255.255.255.0",
                    "role": "intercluster",
                    "use_rest": "never",
                    "vserver": "Cluster_01"
                },
                {
                    "address": "10.0.0.101",
                    "home_node": "Cluster_02-01",
                    "home_port": "e0c",
                    "hostname": "Cluster_02",
                    "interface_name": "ic01",
                    "ipspace": "Default",
                    "netmask": "255.255.255.0",
                    "role": "intercluster",
                    "use_rest": "never",
                    "vserver": "Cluster_02"
                },
                {
                    "address": "10.0.0.126",
                    "home_node": "Cluster_02-01",
                    "home_port": "e0c",
                    "hostname": "ONTAP99_02",
                    "interface_name": "ic02",
                    "ipspace": "Default",
                    "netmask": "255.255.255.0",
                    "role": "intercluster",
                    "use_rest": "never",
                    "vserver": "Cluster_02"
                }
            ],
----

. Define the `ontap_interface` microservice under `cluster_initial` in the `services.yml` file.
+
Copy the following lines to the file to define the microservice:
[source,cli]
+
----
        - name: ontap_interface
          args: ontap_interface
          role: na/ontap_interface
----

. Now that the `ontap_interface` microservice has been defined in the request and the `services.yml` file, run the request again:
+ 
[source, cli]
ansible-playbook -i inventory/hosts  site.yml -e cluster_name=<Cluster_01> -e peer_cluster_name=<Cluster_02>

. Log in to each ONTAP instance and verify that the LIFs have been added. 

== Step 3: Optionally, configure multiple clusters

If required, you can configure multiple clusters in the same request. You must provide variable names for each cluster when you define the request. 

.Steps

. Add an entry for the second cluster under `cluster_initial` in the `tutorial-requests.yml` file to configure both clusters in the same request. 
+
The following example displays the `ontap_aggr` field after the second entry is added.
+
----
   ontap_aggr:
    - hostname:                   "{{ cluster_name }}"
      disk_count:                 24
      name:                       n01_aggr1
      nodes:                      "{{ cluster_name }}-01"
      raid_type:                  raid4

    - hostname:                   "{{ peer_cluster_name }}"
      disk_count:                 24
      name:                       n01_aggr1
      nodes:                      "{{ peer_cluster_name }}-01"
      raid_type:                  raid4
----

. Apply the changes for all other items under `cluster_initial`. 

. Add cluster peering to the request by copying the following lines to the file under `cluster_initial`:
+
----
    ontap_cluster_peer:
    - hostname:                   "{{ cluster_name }}"
      dest_cluster_name:          "{{ cluster_peer }}"
      dest_intercluster_lifs:     "{{ peer_lifs }}"
      source_cluster_name:        "{{ cluster_name }}"
      source_intercluster_lifs:   "{{ cluster_lifs }}"
      peer_options:
        hostname:                 "{{ cluster_peer }}"
----

. Run the Ansible request:
[source, cli]
ansible-playbook -i inventory/hosts -e cluster_name=<Cluster_01>
site.yml -e peer_cluster_name=<Cluster_02> -e cluster_lifs=<cluster_lif_1_IP_address,cluster_lif_2_IP_address>
-e peer_lifs=<peer_lif_1_IP_address,peer_lif_2_IP_address>

== Step 4: Initial SVM configuration

At this stage in the procedure, you configure the SVMs in the cluster.

. Update the `svm_initial` request in the `tutorial-requests.yml` file to configure an SVM and SVM peer relationship. 
+
You must configure the following:
+
* The SVM
* The SVM peer relationship
* The SVM interface for each SVM


. Update the variable definitions in the `svm_initial` request definitions. You must modify the following variable definitions: 
+
* `cluster_name`
* `vserver_name` 
* `peer_cluster_name`
* `peer_vserver`
+
To update the definitions, remove the *‘{}’* after `req_details` for the `svm_initial` definition and add the correct definition.

. After you have properly defined the request, update the `raw_service_request` field at the end of the file to use the `svm_initial` request, as shown in the following example:
+
----
raw_service_request: "{{ svm_initial }}"
----

. Run the request:
[source, cli]
ansible-playbook -i inventory/hosts -e cluster_name=<Cluster_01> -e peer_cluster_name=<Cluster_02> -e peer_vserver=<SVM_02>  -e vserver_name=<SVM_01> site.yml

. Log in to each ONTAP instance and validate the configuration.

. Add the SVM interfaces. 
+ 
Define the `ontap_interface` service under `svm_initial` in the `services.yml` file and run the request again: 
+
[source, cli]
ansible-playbook -i inventory/hosts -e cluster_name=<Cluster_01> -e peer_cluster_name=<Cluster_02> -e peer_vserver=<SVM_02>  -e vserver_name=<SVM_01> site.yml

. Log in to each ONTAP instance and verify that the SVM interfaces have been configured. 

== Step 5: Provision NAS

In the previous steps, the `raw_service_request` is hard-coded. This is useful for learning, development, and testing. You can also dynamically generate a `raw_service_request`. 

The following steps provide an option to dynamically produce the required `raw_service_request` if you do not want to integrate it with higher level systems. 

.Steps 

. Comment out the `raw_service_request` definition at the bottom of the `tutorial-requests.yml` file.
. Navigate to `playbooks/logic-tasks/tutorial.yml`. 

. Define the `logic_operation` variable in the tutorial.yml file.
+
[IMPORTANT]
====
* If the `logic_operation` variable is not defined, the `logic.yml` file does not import any file from the `logic-tasks` folder. This means the `raw_service_request` must be defined outside of Ansible and provided to the framework on execution. 

* A tasks file name in the `logic-tasks` folder must match the value of the `logic_operation` variable and the .yml extension.

* The tasks file in the `logic-tasks` folder dynamically defines a `raw_service_request`. The only requirement is that a valid `raw_service_request` be defined as the last task in the file.
====

. Dynamically define the the service request. 
+
There are multiple ways to apply the logic task to dynamically define a service request. Some of these options are listed below:
+
* Using the Ansible task code within the file.
* Invoking a custom role that returns data suitable for converting to a `raw_service_request` or to the request.
* Invoking another tool outside of the Ansible environment to provide the required data. For example, a REST call to Active IQ Unified Manager.
* Invoking other task files from within the `logic-tasks` folder to organize operations.

The following example commands dynamically define a service request for both clusters:

[source,cli]
ansible-playbook -i inventory/hosts -e cluster2provision=Cluster_01
-e logic_operation=tutorial site.yml

[source,cli]
ansible-playbook -i inventory/hosts -e cluster2provision=Cluster_02
-e logic_operation=tutorial site.yml

== Step 5: Deploy the ONTAP day 0, day 1, and health check services

At this stage you should have already completed the following: 

* Reviewed and modified all files in `playbooks/inventory/group_vars/all` according to your requirements. There are detailed comments in each file to help you make the changes. 
* Added any required task files to the the `logic-tasks` directory.
* Added any required data files to the `playbook/vars` directory.

Use the following commands to deploy the ONTAP day 0, ONTAP day 1, and health check services:

NOTE: The `<temporary_password>` is a temporary password to decrypt the `vault.yml` file and the corresponding vault password.

.Run the ONTAP day 0 service 

[source, cli]
ansible-playbook -i playbooks/inventory/hosts playbooks/site.yml -e logic_operation=cluster_day_0 -e service=cluster_day_0 -vvvv --ask-vault-pass <temporary_password>

.Run the ONTAP day 1 service 
[source, cli]
ansible-playbook -i playbooks/inventory/hosts playbooks/site.yml -e logic_operation=cluster_day_1 -e service=cluster_day_0 -vvvv --ask-vault-pass <temporary_password>

.Apply cluster wide settings

[source,cli]
ansible-playbook -i playbooks/inventory/hosts playbooks/site.yml -e logic_operation=cluster_wide_settings -e service=cluster_wide_settings -vvvv --ask-vault-pass <temporary_password>

.Run health checks

[source,cli]
ansible-playbook -i playbooks/inventory/hosts playbooks/site.yml -e logic_operation=health_checks -e service=health_checks -e enable_health_reports=true -vvvv --ask-vault-pass <temporary_password>
